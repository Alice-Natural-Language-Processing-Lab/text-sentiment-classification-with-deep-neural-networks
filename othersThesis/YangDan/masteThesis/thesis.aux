\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{unsrt}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{finnish}
\@writefile{toc}{\select@language{finnish}}
\@writefile{lof}{\select@language{finnish}}
\@writefile{lot}{\select@language{finnish}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\newlabel{sec:abs}{{}{1}{ABSTRACT}{section*.5}{}}
\newlabel{sec:sym}{{}{2}{ABBREVIATIONS AND SYMBOLS}{section*.6}{}}
\citation{dantone2014body}
\citation{foytik2013two}
\citation{geng2014cvpr}
\citation{hara2014growing}
\citation{HeiChe:2015}
\citation{nillson2014transaction}
\citation{seppa2008transaction}
\citation{Hongsheng2015transaction}
\citation{Chi-Chen2008transaction}
\citation{tongtong2016transaction}
\citation{shunguang2009transaction}
\citation{ozuysal2009pose}
\citation{torki2011regression}
\citation{fenzi2013class}
\citation{hara2014growing}
\citation{torki2011regression}
\citation{fenzi2013class}
\citation{hara2014growing}
\@writefile{toc}{\contentsline {section}{\numberline {1}INTRODUCTION}{3}{section.1}}
\newlabel{sec:introduction}{{1}{3}{INTRODUCTION}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{3}{subsection.1.1}}
\citation{hara2014growing}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The car pose should be estimated as the front direction ($270^\text  {o}$) in the circular output space.\relax }}{4}{figure.1.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:carpose}{{1.1}{4}{The car pose should be estimated as the front direction ($270^\text {o}$) in the circular output space.\relax }{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objectives}{4}{subsection.1.2}}
\citation{cronin2008undertaking}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Research Methodologies}{5}{subsection.1.3}}
\citation{elio2011computing}
\citation{elio2011computing}
\citation{elio2011computing}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Structure of the Thesis}{7}{subsection.1.4}}
\citation{nillson2014transaction}
\citation{seppa2008transaction}
\citation{torki2011regression}
\citation{fenzi2013class}
\citation{hara2014growing}
\citation{ozuysal2009pose}
\citation{glasner2011viewpoint}
\@writefile{toc}{\contentsline {section}{\numberline {2}BACKGROUND}{8}{section.2}}
\newlabel{sec:literatures}{{2}{8}{BACKGROUND}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Car Pose Estimation}{8}{subsection.2.1}}
\newlabel{sec:carpose}{{2.1}{8}{Car Pose Estimation}{subsection.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Abstract pipeline for pose estimation problem: (1) in the training stage, features of training data set with ground-truth are used to train a predictive model; (2) The model is used to predict the viewing angle of a unknown testing image.\relax }}{8}{figure.2.1}}
\newlabel{Fig:pipeline of pose estimation problem}{{2.1}{8}{Abstract pipeline for pose estimation problem: (1) in the training stage, features of training data set with ground-truth are used to train a predictive model; (2) The model is used to predict the viewing angle of a unknown testing image.\relax }{figure.2.1}{}}
\citation{guo08icpr}
\citation{guo08icpr}
\citation{guo08icpr}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Classification vs Regression\relax }}{9}{figure.2.2}}
\newlabel{Fig:classificationandregression}{{2.2}{9}{Classification vs Regression\relax }{figure.2.2}{}}
\citation{guo08icpr}
\citation{guo08icpr}
\citation{foytik2013two}
\citation{HeiChe:2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Challenges}{10}{subsection.2.2}}
\newlabel{subsec:challenges}{{2.2}{10}{Challenges}{subsection.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Illustrative examples of the problem specific characteristics of vision-based car pose estimation. Top: visual differences between different models for the same viewing angle; middle: visual similarity of the neighbouring viewing angles for the same car; bottom: problem-specific circular similarity of flipping pose.\relax }}{11}{figure.2.3}}
\newlabel{Fig:intro}{{2.3}{11}{Illustrative examples of the problem specific characteristics of vision-based car pose estimation. Top: visual differences between different models for the same viewing angle; middle: visual similarity of the neighbouring viewing angles for the same car; bottom: problem-specific circular similarity of flipping pose.\relax }{figure.2.3}{}}
\citation{torki2011regression}
\citation{ozuysal2009pose}
\citation{torki2011regression}
\citation{fenzi2013class}
\citation{hara2014growing}
\citation{ozuysal2009pose}
\citation{cortes1995support}
\citation{guo08icpr}
\citation{hara2014growing}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}State-Of-The-Art Algorithms}{12}{subsection.2.3}}
\citation{torki2011regression}
\citation{fenzi2013class}
\citation{foytik2013two}
\citation{hara2014growing}
\citation{foytik2013two}
\citation{ozuysal2009pose}
\citation{ozuysal2009pose}
\citation{torki2011regression}
\citation{fenzi2013class}
\citation{hara2014growing}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Public Benchmarks}{14}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Examples from the EPFL Multi-view Car Dataset.\relax }}{15}{figure.2.4}}
\newlabel{fig:example}{{2.4}{15}{Examples from the EPFL Multi-view Car Dataset.\relax }{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Summary}{16}{subsection.2.5}}
\citation{breiman2001random}
\citation{bosch2007image}
\citation{sun2012conditional}
\citation{dantone2012real}
\citation{fanelli2011real}
\citation{loh2011classification}
\@writefile{toc}{\contentsline {section}{\numberline {3}Regression Methods}{17}{section.3}}
\newlabel{sec:KRF}{{3}{17}{Regression Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Regression Forests}{17}{subsection.3.1}}
\newlabel{eqn:obj}{{3.1}{17}{Regression Forests}{equation.3.1}{}}
\newlabel{eqn:obj}{{3.2}{17}{Regression Forests}{equation.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Using standard binary splitting rules to construct a regression tree: select a splitting rule from pre-defined set of splitting rule that minimise $SSE$ and then forward training samples into two child nodes according to the selected rule. If stopping criterion is reached then stop splitting. As shown in the left of the figure the input space is first divided into two partitions by the certain splitting rule (green line) and each partition is divided by different rules respectively. Finally the whole input space is divided into $4$ partitions ($K=4$). The tree structure is shown in the right side. The output of each leaf can be represented as $a_i = \frac  {1}{n_i}\DOTSB \sum@ \slimits@ _{x_i \in \mathcal  {R}_i}y_i$ with $n_i$ denoting the number of training samples that are forwarded into the input subspace $\mathcal  {R}_i$. \relax }}{18}{figure.3.1}}
\newlabel{Fig:regressiontree}{{3.1}{18}{Using standard binary splitting rules to construct a regression tree: select a splitting rule from pre-defined set of splitting rule that minimise $SSE$ and then forward training samples into two child nodes according to the selected rule. If stopping criterion is reached then stop splitting. As shown in the left of the figure the input space is first divided into two partitions by the certain splitting rule (green line) and each partition is divided by different rules respectively. Finally the whole input space is divided into $4$ partitions ($K=4$). The tree structure is shown in the right side. The output of each leaf can be represented as $a_i = \frac {1}{n_i}\sum _{x_i \in \mathcal {R}_i}y_i$ with $n_i$ denoting the number of training samples that are forwarded into the input subspace $\mathcal {R}_i$. \relax }{figure.3.1}{}}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{ozuysal2009pose}
\citation{torki2011regression}
\citation{fenzi2013class}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}K-clustering Regression Forest}{19}{subsection.3.2}}
\citation{dalal2005histograms}
\citation{fenzi2013class}
\citation{hara2014growing}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Three steps of K-clusters splitting method ($K=3$) for regression tree construction: (1) Cluster the training data into 3 groups by applying K-means on the target space (left). (2) Divide the input space into $3$ subspaces by SVM which can preserve the found clusters as much as possible (middle) and forward training samples into three child nodes. (3) A constant estimate is calculated for each subspace if no more partitioning is needed. Some misclassification is unavoidable shown as some data points that changed color (right).\relax }}{20}{figure.3.2}}
\newlabel{Fig:K-clusters Regression Tree}{{3.2}{20}{Three steps of K-clusters splitting method ($K=3$) for regression tree construction: (1) Cluster the training data into 3 groups by applying K-means on the target space (left). (2) Divide the input space into $3$ subspaces by SVM which can preserve the found clusters as much as possible (middle) and forward training samples into three child nodes. (3) A constant estimate is calculated for each subspace if no more partitioning is needed. Some misclassification is unavoidable shown as some data points that changed color (right).\relax }{figure.3.2}{}}
\newlabel{verification}{{3.1}{20}{Evaluation of the KRF and AKRF with the EPFL Multi-View Car dataset.\relax }{table.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  Evaluation of the KRF and AKRF with the EPFL Multi-View Car dataset.\relax }}{20}{table.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experimental Verification}{20}{subsection.3.3}}
\citation{hara2014growing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Summary}{21}{subsection.3.4}}
\citation{guo08icpr}
\citation{ding2016articulated}
\citation{kong2015head}
\citation{fanelli2011real}
\@writefile{toc}{\contentsline {section}{\numberline {4}Part-Aware Target Coding}{22}{section.4}}
\newlabel{sec:visibleparts}{{4}{22}{Part-Aware Target Coding}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Introduction}{22}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A sequence of images from EPFL Multi-View Car benchmarks. Each image is highlighted by semantic visible parts.\relax }}{23}{figure.4.1}}
\newlabel{fig:intro}{{4.1}{23}{A sequence of images from EPFL Multi-View Car benchmarks. Each image is highlighted by semantic visible parts.\relax }{figure.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Working flow of the proposed framework.\relax }}{24}{figure.4.2}}
\newlabel{fig:workflow}{{4.2}{24}{Working flow of the proposed framework.\relax }{figure.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Pipeline}{25}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces 16 parts were manually labelled for all the image in dataset and SVM classifiers were trained independently for 16 parts of car: (1) front license number; (2) back license number; (3) front logo; (5) front left light; (6) front right light; (7) back left light; (8) back right light; (9) front left wheel; (10) front right wheel; (11) back left wheel; (12) back right wheel; (13) left mirror; (14) right mirror; (15) left door(s); (16) right door(s). When the HOG feature is feed to the classifiers probabilities of the 16 parts will be predicted. As shown in the right side, there are 5 parts are predicted to be highly probable with higher than $0.95$ for the parts labelled with red numbers: (10) front right wheel, (12) back right wheel and (14) right mirror. \relax }}{25}{figure.4.3}}
\newlabel{fig:parts}{{4.3}{25}{16 parts were manually labelled for all the image in dataset and SVM classifiers were trained independently for 16 parts of car: (1) front license number; (2) back license number; (3) front logo; (5) front left light; (6) front right light; (7) back left light; (8) back right light; (9) front left wheel; (10) front right wheel; (11) back left wheel; (12) back right wheel; (13) left mirror; (14) right mirror; (15) left door(s); (16) right door(s). When the HOG feature is feed to the classifiers probabilities of the 16 parts will be predicted. As shown in the right side, there are 5 parts are predicted to be highly probable with higher than $0.95$ for the parts labelled with red numbers: (10) front right wheel, (12) back right wheel and (14) right mirror. \relax }{figure.4.3}{}}
\citation{cortes1995support}
\citation{chang2011libsvm}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Visible Semantic Parts}{26}{subsection.4.3}}
\newlabel{sec:visible_parts}{{4.3}{26}{Visible Semantic Parts}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Part-Aware Target Coding}{26}{subsection.4.4}}
\newlabel{sec:target_coding}{{4.4}{26}{Part-Aware Target Coding}{subsection.4.4}{}}
\citation{chen2016pedestrian}
\citation{cortes1995support}
\citation{chang2011libsvm}
\citation{hara2014growing}
\newlabel{eq:svm1}{{4.2}{27}{Part-Aware Target Coding}{equation.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Regression Mapping onto Viewpoint Space}{27}{subsection.4.5}}
\newlabel{sec:PATC_Regressor}{{4.5}{27}{Regression Mapping onto Viewpoint Space}{subsection.4.5}{}}
\newlabel{eqn:obj1}{{4.3}{27}{Regression Mapping onto Viewpoint Space}{equation.4.3}{}}
\citation{hara2014growing}
\citation{kashyap1977bayesian}
\citation{schwarz1978estimating}
\citation{ozuysal2009pose}
\citation{ozuysal2009pose}
\citation{rosipal2002kernel}
\citation{cortes1995support}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\newlabel{eqn:obj_circular}{{4.6}{28}{Regression Mapping onto Viewpoint Space}{equation.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Experiments}{28}{subsection.4.6}}
\citation{ozuysal2009pose}
\citation{torki2011regression}
\citation{fenzi2013class}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\newlabel{tab.state-of-the-art1}{{4.1}{29}{Comparative evaluation of the state-of-the-art methods with the EPFL Multi-View Car dataset.\relax }{table.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces  Comparative evaluation of the state-of-the-art methods with the EPFL Multi-View Car dataset.\relax }}{29}{table.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparison of PAR (ours), KRF and AKRF on mae ($100\%$) for each sequence (leave-one-sequence-out).\relax }}{30}{figure.4.4}}
\newlabel{fig:compare_sequence}{{4.4}{30}{Comparison of PAR (ours), KRF and AKRF on mae ($100\%$) for each sequence (leave-one-sequence-out).\relax }{figure.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Summary}{30}{subsection.4.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The top row shows the cars in sequences 7, 9 and 10 on which our methods reducing mae \relax }}{31}{figure.4.5}}
\newlabel{fig:ill_example}{{4.5}{31}{The top row shows the cars in sequences 7, 9 and 10 on which our methods reducing mae \relax }{figure.4.5}{}}
\citation{dantone2014body}
\citation{dantone2012real}
\citation{foytik2013two}
\citation{liu2015age}
\@writefile{toc}{\contentsline {section}{\numberline {5}Hierarchical Sliding Slice Regression }{32}{section.5}}
\newlabel{sec:HRRS}{{5}{32}{Hierarchical Sliding Slice Regression}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Introduction}{32}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Intuitive concept of the proposed HSSR.\relax }}{33}{figure.5.1}}
\newlabel{fig:concept}{{5.1}{33}{Intuitive concept of the proposed HSSR.\relax }{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Our approach for estimating car pose hierarchically by first coarse grouping via classification and then fine estimation via regression.\relax }}{34}{figure.5.2}}
\newlabel{Fig:pipeline}{{5.2}{34}{Our approach for estimating car pose hierarchically by first coarse grouping via classification and then fine estimation via regression.\relax }{figure.5.2}{}}
\citation{cortes1995support}
\citation{dantone2014body}
\citation{dantone2012real}
\citation{foytik2013two}
\citation{liu2015age}
\citation{dantone2014body}
\citation{dantone2012real}
\citation{foytik2013two}
\citation{liu2015age}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Pipeline}{35}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Circular Slice Construction}{35}{subsection.5.3}}
\newlabel{sec:circular_slice}{{5.3}{35}{Circular Slice Construction}{subsection.5.3}{}}
\citation{zelnik2004self}
\citation{chang2011libsvm}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Coarse Classifier}{36}{subsection.5.4}}
\newlabel{sec:coarse_classifier}{{5.4}{36}{Coarse Classifier}{subsection.5.4}{}}
\citation{cortes1995support}
\citation{chang2011libsvm}
\citation{fanelli_IJCV}
\citation{huang2010head}
\citation{demaris1995tutorial}
\citation{ozuysal2009pose}
\citation{chen2011human}
\citation{orozco2009head}
\citation{munoz2012multi}
\citation{breiman2001random}
\citation{hara2014growing}
\citation{hara2014growing}
\newlabel{eq:svm}{{5.2}{37}{Coarse Classifier}{equation.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Fine Slice-Specific Regressor}{37}{subsection.5.5}}
\newlabel{sec:regressor}{{5.5}{37}{Fine Slice-Specific Regressor}{subsection.5.5}{}}
\newlabel{eqn:obj}{{5.3}{37}{Fine Slice-Specific Regressor}{equation.5.3}{}}
\citation{hara2014growing}
\citation{kashyap1977bayesian}
\citation{schwarz1978estimating}
\citation{ozuysal2009pose}
\citation{rosipal2002kernel}
\citation{cortes1995support}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\newlabel{eqn:obj_circular}{{5.5}{38}{Fine Slice-Specific Regressor}{equation.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Experiments}{38}{subsection.5.6}}
\citation{ozuysal2009pose}
\citation{torki2011regression}
\citation{fenzi2013class}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{hara2014growing}
\citation{geng2007automatic}
\newlabel{tab.state-of-the-arts}{{5.1}{39}{Comparative evaluation of the state-of-the-art methods with the EPFL Multi-View Car dataset.\relax }{table.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces  Comparative evaluation of the state-of-the-art methods with the EPFL Multi-View Car dataset.\relax }}{39}{table.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparison of HSSR (ours), KRF and AKRF on mae (100\%) for each sequence (leave-one-sequence-out).\relax }}{40}{figure.5.3}}
\newlabel{Figure1}{{5.3}{40}{Comparison of HSSR (ours), KRF and AKRF on mae (100\%) for each sequence (leave-one-sequence-out).\relax }{figure.5.3}{}}
\newlabel{tab.overlapping}{{5.2}{40}{Evaluation on the circular slice step size proportional to the $45^\circ $ slice size.\relax }{table.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces  Evaluation on the circular slice step size proportional to the $45^\circ $ slice size.\relax }}{40}{table.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Evaluation on the circular slice size. \relax }}{40}{table.5.3}}
\newlabel{tab.size}{{5.3}{40}{Evaluation on the circular slice size. \relax }{table.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparative cumulative scores. The higher the better.\relax }}{41}{figure.5.4}}
\newlabel{Fig:cum}{{5.4}{41}{Comparative cumulative scores. The higher the better.\relax }{figure.5.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {even split}}}{41}{subfigure.4.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {leave-one-sequence-out}}}{41}{subfigure.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Summary}{41}{subsection.5.7}}
\citation{dan2017hssr}
\@writefile{toc}{\contentsline {section}{\numberline {6}CONCLUSION}{42}{section.6}}
\newlabel{sec:conclusion}{{6}{42}{CONCLUSION}{section.6}{}}
\bibdata{resources/HeadPose}
\bibcite{dantone2014body}{1}
\bibcite{foytik2013two}{2}
\bibcite{geng2014cvpr}{3}
\bibcite{hara2014growing}{4}
\bibcite{HeiChe:2015}{5}
\bibcite{nillson2014transaction}{6}
\bibcite{seppa2008transaction}{7}
\bibcite{Hongsheng2015transaction}{8}
\bibcite{Chi-Chen2008transaction}{9}
\bibcite{tongtong2016transaction}{10}
\bibcite{shunguang2009transaction}{11}
\@writefile{toc}{\contentsline {section}{REFERENCES}{43}{section.6}}
\bibcite{ozuysal2009pose}{12}
\bibcite{torki2011regression}{13}
\bibcite{fenzi2013class}{14}
\bibcite{cronin2008undertaking}{15}
\bibcite{elio2011computing}{16}
\bibcite{glasner2011viewpoint}{17}
\bibcite{guo08icpr}{18}
\bibcite{cortes1995support}{19}
\bibcite{breiman2001random}{20}
\bibcite{bosch2007image}{21}
\bibcite{sun2012conditional}{22}
\bibcite{dantone2012real}{23}
\bibcite{fanelli2011real}{24}
\bibcite{loh2011classification}{25}
\bibcite{dalal2005histograms}{26}
\bibcite{ding2016articulated}{27}
\bibcite{kong2015head}{28}
\bibcite{chang2011libsvm}{29}
\bibcite{chen2016pedestrian}{30}
\bibcite{kashyap1977bayesian}{31}
\bibcite{schwarz1978estimating}{32}
\bibcite{rosipal2002kernel}{33}
\bibcite{liu2015age}{34}
\bibcite{zelnik2004self}{35}
\bibcite{fanelli_IJCV}{36}
\bibcite{huang2010head}{37}
\bibcite{demaris1995tutorial}{38}
\bibcite{chen2011human}{39}
\bibcite{orozco2009head}{40}
\bibcite{munoz2012multi}{41}
\bibcite{dan2017hssr}{42}
\newlabel{LastPage}{{}{46}{}{page.46}{}}
\xdef\lastpage@lastpage{46}
\xdef\lastpage@lastpageHy{46}
