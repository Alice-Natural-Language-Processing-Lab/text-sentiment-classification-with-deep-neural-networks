\select@language {finnish}
\select@language {english}
\contentsline {figure}{\numberline {1.1}{\ignorespaces The car pose should be estimated as the front direction ($270^\text {o}$) in the circular output space.\relax }}{4}{figure.1.1}
\contentsline {figure}{\numberline {2.1}{\ignorespaces Abstract pipeline for pose estimation problem: (1) in the training stage, features of training data set with ground-truth are used to train a predictive model; (2) The model is used to predict the viewing angle of a unknown testing image.\relax }}{8}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Classification vs Regression\relax }}{9}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Illustrative examples of the problem specific characteristics of vision-based car pose estimation. Top: visual differences between different models for the same viewing angle; middle: visual similarity of the neighbouring viewing angles for the same car; bottom: problem-specific circular similarity of flipping pose.\relax }}{11}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Examples from the EPFL Multi-view Car Dataset.\relax }}{15}{figure.2.4}
\contentsline {figure}{\numberline {3.1}{\ignorespaces Using standard binary splitting rules to construct a regression tree: select a splitting rule from pre-defined set of splitting rule that minimise $SSE$ and then forward training samples into two child nodes according to the selected rule. If stopping criterion is reached then stop splitting. As shown in the left of the figure the input space is first divided into two partitions by the certain splitting rule (green line) and each partition is divided by different rules respectively. Finally the whole input space is divided into $4$ partitions ($K=4$). The tree structure is shown in the right side. The output of each leaf can be represented as $a_i = \frac {1}{n_i}\DOTSB \sum@ \slimits@ _{x_i \in \mathcal {R}_i}y_i$ with $n_i$ denoting the number of training samples that are forwarded into the input subspace $\mathcal {R}_i$. \relax }}{18}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Three steps of K-clusters splitting method ($K=3$) for regression tree construction: (1) Cluster the training data into 3 groups by applying K-means on the target space (left). (2) Divide the input space into $3$ subspaces by SVM which can preserve the found clusters as much as possible (middle) and forward training samples into three child nodes. (3) A constant estimate is calculated for each subspace if no more partitioning is needed. Some misclassification is unavoidable shown as some data points that changed color (right).\relax }}{20}{figure.3.2}
\contentsline {figure}{\numberline {4.1}{\ignorespaces A sequence of images from EPFL Multi-View Car benchmarks. Each image is highlighted by semantic visible parts.\relax }}{23}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Working flow of the proposed framework.\relax }}{24}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces 16 parts were manually labelled for all the image in dataset and SVM classifiers were trained independently for 16 parts of car: (1) front license number; (2) back license number; (3) front logo; (5) front left light; (6) front right light; (7) back left light; (8) back right light; (9) front left wheel; (10) front right wheel; (11) back left wheel; (12) back right wheel; (13) left mirror; (14) right mirror; (15) left door(s); (16) right door(s). When the HOG feature is feed to the classifiers probabilities of the 16 parts will be predicted. As shown in the right side, there are 5 parts are predicted to be highly probable with higher than $0.95$ for the parts labelled with red numbers: (10) front right wheel, (12) back right wheel and (14) right mirror. \relax }}{25}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparison of PAR (ours), KRF and AKRF on mae ($100\%$) for each sequence (leave-one-sequence-out).\relax }}{30}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces The top row shows the cars in sequences 7, 9 and 10 on which our methods reducing mae \relax }}{31}{figure.4.5}
\contentsline {figure}{\numberline {5.1}{\ignorespaces Intuitive concept of the proposed HSSR.\relax }}{33}{figure.5.1}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Our approach for estimating car pose hierarchically by first coarse grouping via classification and then fine estimation via regression.\relax }}{34}{figure.5.2}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparison of HSSR (ours), KRF and AKRF on mae (100\%) for each sequence (leave-one-sequence-out).\relax }}{40}{figure.5.3}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparative cumulative scores. The higher the better.\relax }}{41}{figure.5.4}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {even split}}}{41}{subfigure.4.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {leave-one-sequence-out}}}{41}{subfigure.4.2}
