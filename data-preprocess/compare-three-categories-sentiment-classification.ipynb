{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset from \"tripadvisor Paris\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from random import randint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first traditional rule-based classifcation method\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to the db\n",
    "client = MongoClient()\n",
    "db = client.sentimentAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'sentimentAnalysis')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCollection(colletName = \"\"):\n",
    "    '''\n",
    "    return pandas dataframe.\n",
    "    '''\n",
    "    cursor = db[collName].find({})\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take Barcelona city hotel reviews as example\n",
    "cityName = \"barcelonaTripadvisor\"\n",
    "data = getCollection(colletName = cityName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_sentiment_category(row):\n",
    "    \"\"\"\n",
    "    split reviews into two categories, pos, labled by \"1\", neg, labeled by \"0\"\n",
    "    [0, 1, 2] is neg, [4,5] is pos, \n",
    "    [3] is neutral, it needs to be deleted since we only do two-class classification\n",
    "    \"\"\"\n",
    "    if row[\"score\"] in [0, 1, 2]:\n",
    "        return 0\n",
    "    if row[\"score\"] in [3]:\n",
    "        return -1\n",
    "    if row[\"score\"] in [4, 5]:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['sentiment'] = data.apply(lambda row: label_sentiment_category(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hotelLocation</th>\n",
       "      <th>hotelName</th>\n",
       "      <th>hotelStars</th>\n",
       "      <th>hotelUrl</th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>userId</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5987192c9b1f26681686008e</td>\n",
       "      <td>July 31, 2017</td>\n",
       "      <td>Carrer Elisabets 11, 08001 Barcelona, Spain</td>\n",
       "      <td>Casa Camper Hotel Barcelona</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.tripadvisor.com/Hotel_Review-g1874...</td>\n",
       "      <td>It truly felt like coming home when we came th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SO SO Happy to Be BACK!</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>Lili1012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5987192e9b1f26681686008f</td>\n",
       "      <td>July 26, 2017</td>\n",
       "      <td>Carrer Elisabets 11, 08001 Barcelona, Spain</td>\n",
       "      <td>Casa Camper Hotel Barcelona</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.tripadvisor.com/Hotel_Review-g1874...</td>\n",
       "      <td>Perfect place to stay in Barcelona.. We spent ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>AimamKaur</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id           date  \\\n",
       "0  5987192c9b1f26681686008e  July 31, 2017   \n",
       "1  5987192e9b1f26681686008f  July 26, 2017   \n",
       "\n",
       "                                 hotelLocation                      hotelName  \\\n",
       "0  Carrer Elisabets 11, 08001 Barcelona, Spain   Casa Camper Hotel Barcelona    \n",
       "1  Carrer Elisabets 11, 08001 Barcelona, Spain   Casa Camper Hotel Barcelona    \n",
       "\n",
       "  hotelStars                                           hotelUrl  \\\n",
       "0        4.0  https://www.tripadvisor.com/Hotel_Review-g1874...   \n",
       "1        4.0  https://www.tripadvisor.com/Hotel_Review-g1874...   \n",
       "\n",
       "                                              review  score  \\\n",
       "0  It truly felt like coming home when we came th...    5.0   \n",
       "1  Perfect place to stay in Barcelona.. We spent ...    5.0   \n",
       "\n",
       "                     title                                                url  \\\n",
       "0  SO SO Happy to Be BACK!  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "1                  Perfect  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "\n",
       "      userId  sentiment  \n",
       "0   Lili1012          1  \n",
       "1  AimamKaur          1  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"data size : \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove row where sentiment is neutral\n",
    "data = data[data.sentiment != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"data shape after remove neutral reviews : \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_punc_and_marks(row):\n",
    "    \"\"\"\n",
    "    remove punctuation, including ???????\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(row[\"review\"])\n",
    "\n",
    "    words=[word.lower() for word in words if word.isalpha()]\n",
    "    words = words[:250]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['review'] = data.apply(lambda row: clean_punc_and_marks(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[\"review\"]\n",
    "y = data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"check the consistent size of reviews and sentiment : \", \"review size : \", len(X), \"sentiment size: \", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    it truly felt like coming home when we came th...\n",
       "1    perfect place to stay in we spent nights there...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data[:2][\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in the future, using cross_validation technology\n",
    "#reference paper\n",
    "# Pang, B., Lee, L. and Vaithyanathan, S., 2002, July. \n",
    "#Thumbs up?: sentiment classification using machine learning techniques. \n",
    "#In Proceedings of the ACL-02 conference on Empirical methods in natural \n",
    "#language processing-Volume 10 (pp. 79-86). Association for Computational Linguistics.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2186"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"size shape for train data : \", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanSentences(string):\n",
    "    \"\"\"\n",
    "    removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "    \"\"\"\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lstm():\n",
    "    \"\"\"\n",
    "    train lstm model with hotel reviews\n",
    "    \"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clf_VADER():\n",
    "    \"\"\"\n",
    "    reviews sentiment classification on VADER package\n",
    "    \"\"\"\n",
    "    y_pred_VADER = list()\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for i in X_test:\n",
    "        vs = analyzer.polarity_scores(i)\n",
    "        if vs[\"compound\"] > 0:\n",
    "            y_pred_VADER.append(1)\n",
    "        if vs[\"compound\"] < 0:\n",
    "            y_pred_VADER.append(0)\n",
    "#         if the predicted review is neutral, then we set it as positive since we only have two categories\n",
    "        if vs[\"compound\"] == 0:\n",
    "             y_pred_VADER.append(1)\n",
    "    return y_pred_VADER\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def clf_SVM():\n",
    "    \"\"\"\n",
    "    train the model with stastical machine learning methods, here is linear SVC\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "        ('clf', LinearSVC(C=1000)),\n",
    "    ])\n",
    "\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # TASK: print the mean and std for each candidate along with the parameter\n",
    "    # settings for all the candidates explored by grid search.\n",
    "    n_candidates = len(grid_search.cv_results_['params'])\n",
    "    for i in range(n_candidates):\n",
    "        print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "                 % (grid_search.cv_results_['params'][i],\n",
    "                    grid_search.cv_results_['mean_test_score'][i],\n",
    "                    grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
    "    # named y_predicted\n",
    "    y_predicted = grid_search.predict(X_test)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(metrics.classification_report(y_test, y_predicted,\n",
    "                                        target_names=None))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"elapsed time: \", round(elapsed_time, 2), \" seconds\")\n",
    "\n",
    "    # Print and plot the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "    \n",
    "    plt.matshow(cm)\n",
    "    plt.show()\n",
    "    return cmtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clf_lstm():\n",
    "    \"\"\"\n",
    "    using Long short time memory recurrent neural network model to do sentiment classification\n",
    "    \"\"\"\n",
    "    # first step, load pre-trained model from others\n",
    "    ids = np.load('idsMatrix.npy')\n",
    "    wordVectors = np.load('./wordVectors.npy')\n",
    "    wordsList = np.load('./wordsList.npy')\n",
    "    print('Loaded the word list!')\n",
    "    wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "    wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "    input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "    data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.25)\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "    \n",
    "    y_pred = list()\n",
    "    for i in X_test:\n",
    "        inputMatrix = getSentenceMatrix(i)\n",
    "        predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "        if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "#         print \"Positive Sentiment\"\n",
    "            y_pred.append(1)\n",
    "        if (predictedSentiment[0] < predictedSentiment[1]):\n",
    "#         print \"Positive Sentiment\"\n",
    "            y_pred.append(0)\n",
    "        if (predictedSentiment[0] == predictedSentiment[1]):\n",
    "#         print \"neural Sentiment\"\n",
    "            y_pred.append(1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.97; std - 0.01\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.96; std - 0.00\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.68      0.77        56\n",
      "          1       0.96      0.99      0.98       491\n",
      "\n",
      "avg / total       0.96      0.96      0.96       547\n",
      "\n",
      "elapsed time:  2.74  seconds\n",
      "[[ 38  18]\n",
      " [  5 486]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test linear SVC algorithm\n",
    "svm_clf = clf_SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-90000\n",
      "elapsed time:  44.08  seconds for lstm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.73      0.58        56\n",
      "          1       0.97      0.91      0.94       491\n",
      "\n",
      "avg / total       0.92      0.89      0.90       547\n",
      "\n",
      "[[ 41  15]\n",
      " [ 45 446]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4vrFZ3yo5I4PhYQPhYQPhYQPhYQPhf4DRbmvVXQTcVUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time_LSTM = time.time()\n",
    "y_pred_lstm = clf_lstm()\n",
    "elapsed_time_LSTM = time.time() - start_time\n",
    "print(\"elapsed time: \", round(elapsed_time_LSTM, 2), \" seconds for lstm\")\n",
    "# Print and plot the confusion matrix\n",
    "cm_lstm = metrics.confusion_matrix(y_test, y_pred_lstm)\n",
    "print(metrics.classification_report(y_test, y_pred_lstm,target_names=None))\n",
    "print(cm_lstm)\n",
    "\n",
    "plt.matshow(cm_lstm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time:  0.73  seconds for VADER\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.54      0.66        56\n",
      "          1       0.95      0.99      0.97       491\n",
      "\n",
      "avg / total       0.94      0.94      0.94       547\n",
      "\n",
      "[[ 30  26]\n",
      " [  5 486]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "y_pred_VADER = clf_VADER()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time: \", round(elapsed_time, 2), \" seconds for VADER\")\n",
    "# Print and plot the confusion matrix\n",
    "cm_VADER = metrics.confusion_matrix(y_test, y_pred_VADER)\n",
    "print(metrics.classification_report(y_test, y_pred_VADER,target_names=None))\n",
    "print(cm_VADER)\n",
    "\n",
    "plt.matshow(cm_VADER)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neu': 0.254, 'neg': 0.0, 'pos': 0.746, 'compound': 0.8316}\n",
      "{'neu': 0.354, 'neg': 0.646, 'pos': 0.0, 'compound': -0.7424}\n",
      "{'neu': 0.248, 'neg': 0.0, 'pos': 0.752, 'compound': 0.8439}\n",
      "{'neu': 0.299, 'neg': 0.0, 'pos': 0.701, 'compound': 0.8545}\n",
      "{'neu': 0.246, 'neg': 0.0, 'pos': 0.754, 'compound': 0.9227}\n",
      "{'neu': 0.233, 'neg': 0.0, 'pos': 0.767, 'compound': 0.9342}\n",
      "{'neu': 0.294, 'neg': 0.0, 'pos': 0.706, 'compound': 0.9469}\n",
      "{'neu': 0.508, 'neg': 0.0, 'pos': 0.492, 'compound': 0.4404}\n",
      "{'neu': 0.657, 'neg': 0.0, 'pos': 0.343, 'compound': 0.3832}\n",
      "{'neu': 0.579, 'neg': 0.327, 'pos': 0.094, 'compound': -0.7042}\n",
      "{'neu': 0.637, 'neg': 0.0, 'pos': 0.363, 'compound': 0.431}\n",
      "{'neu': 0.294, 'neg': 0.0, 'pos': 0.706, 'compound': 0.8633}\n",
      "{'neu': 0.221, 'neg': 0.779, 'pos': 0.0, 'compound': -0.5461}\n",
      "{'neu': 0.569, 'neg': 0.179, 'pos': 0.251, 'compound': 0.2228}\n"
     ]
    }
   ],
   "source": [
    "# --- examples -------\n",
    "sentences = [\"VADER is smart, handsome, and funny.\",      # positive sentence example\n",
    "            \"VADER is not smart, handsome, nor funny.\",   # negation sentence example\n",
    "            \"VADER is smart, handsome, and funny!\",       # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "            \"VADER is very smart, handsome, and funny.\",  # booster words handled correctly (sentiment intensity adjusted)\n",
    "            \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "            \"VADER is VERY SMART, handsome, and FUNNY!!!\",# combination of signals - VADER appropriately adjusts intensity\n",
    "            \"VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\",# booster words & punctuation make this close to ceiling for score\n",
    "            \"The book was good.\",                                     # positive sentence\n",
    "            \"The book was kind of good.\",                 # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "            \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "            \"At least it isn't a horrible book.\",         # negated negative sentence with contraction\n",
    "            \"Make sure you :) or :D today!\",              # emoticons handled\n",
    "            \"Today SUX!\",                                 # negative slang with capitalization emphasis\n",
    "            \"Today only kinda sux! But I'll get by, lol\"  # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "             ]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    if vs[\"compound\"] > 0:\n",
    "        \n",
    "    print(vs)\n",
    "#     print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# train this model with my own train sample data, lstm model\n",
    "def clf_neuralnetwork_first():\n",
    "    numDimensions = 300\n",
    "    maxSeqLength = 250\n",
    "    batchSize = 24\n",
    "    lstmUnits = 64\n",
    "    numClasses = 2\n",
    "    iterations = 100000\n",
    "    \n",
    "   \n",
    "    wordsList = np.load('wordsList.npy').tolist()\n",
    "    wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "    wordVectors = np.load('wordVectors.npy')\n",
    "    \n",
    "   \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "    input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "    data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.25)\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "    \n",
    "    # Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "    \n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "    def cleanSentences(string):\n",
    "        string = string.lower().replace(\"<br />\", \" \")\n",
    "        return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "    def getSentenceMatrix(sentence):\n",
    "        arr = np.zeros([batchSize, maxSeqLength])\n",
    "        sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "        cleanedSentence = cleanSentences(sentence)\n",
    "        split = cleanedSentence.split()\n",
    "        for indexCounter,word in enumerate(split):\n",
    "            try:\n",
    "                sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "        return sentenceMatrix\n",
    "    y_pred = list()\n",
    "    for i in X_test_clean_punc:\n",
    "        inputMatrix = getSentenceMatrix(i)\n",
    "        predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "        if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "#         print \"Positive Sentiment\"\n",
    "            y_pred.append(1)\n",
    "        if (predictedSentiment[0] < predictedSentiment[1]):\n",
    "#         print \"Positive Sentiment\"\n",
    "            y_pred.append(0)\n",
    "        if (predictedSentiment[0] == predictedSentiment[1]):\n",
    "#         print \"neural Sentiment\"\n",
    "            y_pred.append(1)\n",
    "    return y_pred\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print and plot the confusion matrix\n",
    "cm_lstm = metrics.confusion_matrix(y_test, y_pred_lstm)\n",
    "print(cm_lstm)\n",
    "\n",
    "plt.matshow(cm_lstm)\n",
    "plt.show()\n",
    "return cm_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clf_machinelearning_second(data):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 10\n",
    "maxSeqLength = 250\n",
    "# maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-82-297707c63317>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n"
     ]
    }
   ],
   "source": [
    "wordsList = np.load('./wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels = getTrainBatch();\n",
    "   sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "   #Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "       summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "       writer.add_summary(summary, i)\n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "   if (i % 10000 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_clean_punc = list()\n",
    "for review in X_test:\n",
    "    X_test_clean_punc.append(remove_marks(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label test hotel reviews as positive and negative\n",
    "y_test_posneg = list()\n",
    "for i in y_test:\n",
    "    if i in [3,4,5]:\n",
    "        y_test_posneg.append(1)\n",
    "    if i in [0, 1,2]:\n",
    "        y_test_posneg.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "\n",
    "\n",
    "def remove_marks(s):\n",
    "    words = nltk.word_tokenize(s)\n",
    "\n",
    "    words=[word.lower() for word in words if word.isalpha()]\n",
    "    words = words[:250]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-90000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.70      0.47        56\n",
      "          1       0.97      0.87      0.92       559\n",
      "\n",
      "avg / total       0.91      0.86      0.88       615\n",
      "\n",
      "elapsed time:  0.0  seconds for lstm\n"
     ]
    }
   ],
   "source": [
    "y_pred_lstm = clf_neuralnetwork_first()\n",
    "\n",
    "start_time = time.time()\n",
    "print(metrics.classification_report(y_test_posneg, y_pred_lstm,target_names=None))\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time: \", round(elapsed_time, 2), \" seconds for lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
